import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier


# Load the dataset
url = 'https://raw.githubusercontent.com/arthurlnlt/machinelearning/refs/heads/main/heart.csv'
data = pd.read_csv(url)

# Display the first few rows of the dataset
print(data.head())

# Display information about the dataset
print(data.info())

# Display descriptive statistics
print(data.describe())

# Visualize data distributions
sns.pairplot(data, hue='target')
plt.show()

# Identify missing values
print(data.isnull().sum())

# Visualize outliers
sns.boxplot(data=data)
plt.xticks(rotation=90)
plt.show()

# Calculate summary statistics
print(data.describe())


# Separate features and target
X = data.drop('target', axis=1)
y = data['target']

# Identify numeric and categorical features
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Create pipelines for preprocessing
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Apply preprocessing
X_preprocessed = preprocessor.fit_transform(X)

# Display preprocessed data
print(X_preprocessed)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
log_reg = LogisticRegression()

# Define the hyperparameters to tune
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs']
}

# Perform GridSearchCV to find the best hyperparameters
grid_search_log_reg = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')
grid_search_log_reg.fit(X_train, y_train)

# Best parameters and model
best_log_reg = grid_search_log_reg.best_estimator_
print("Best parameters for Logistic Regression:", grid_search_log_reg.best_params_)

# Evaluate the model
y_pred_log_reg = best_log_reg.predict(X_test)
print("Classification Report for Logistic Regression:\n", classification_report(y_test, y_pred_log_reg))
print("Confusion Matrix for Logistic Regression:\n", confusion_matrix(y_test, y_pred_log_reg))

# ROC curve
fpr, tpr, _ = roc_curve(y_test, best_log_reg.predict_proba(X_test)[:,1])
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Initialize the Decision Tree model
dec_tree = DecisionTreeClassifier()

# Define the hyperparameters to tune
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform GridSearchCV to find the best hyperparameters
grid_search_dec_tree = GridSearchCV(dec_tree, param_grid, cv=5, scoring='accuracy')
grid_search_dec_tree.fit(X_train, y_train)

# Best parameters and model
best_dec_tree = grid_search_dec_tree.best_estimator_
print("Best parameters for Decision Tree:", grid_search_dec_tree.best_params_)

# Evaluate the model
y_pred_dec_tree = best_dec_tree.predict(X_test)
print("Classification Report for Decision Tree:\n", classification_report(y_test, y_pred_dec_tree))
print("Confusion Matrix for Decision Tree:\n", confusion_matrix(y_test, y_pred_dec_tree))

# ROC curve
fpr, tpr, _ = roc_curve(y_test, best_dec_tree.predict_proba(X_test)[:,1])
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

Random Forest

from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
rand_forest = RandomForestClassifier()

# Define the hyperparameters to tune
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform GridSearchCV to find the best hyperparameters
grid_search_rand_forest = GridSearchCV(rand_forest, param_grid, cv=5, scoring='accuracy')
grid_search_rand_forest.fit(X_train, y_train)

# Best parameters and model
best_rand_forest = grid_search_rand_forest.best_estimator_
print("Best parameters for Random Forest:", grid_search_rand_forest.best_params_)

# Evaluate the model
y_pred_rand_forest = best_rand_forest.predict(X_test)
print("Classification Report for Random Forest:\n", classification_report(y_test, y_pred_rand_forest))
print("Confusion Matrix for Random Forest:\n", confusion_matrix(y_test, y_pred_rand_forest))

# ROC curve
fpr, tpr, _ = roc_curve(y_test, best_rand_forest.predict_proba(X_test)[:,1])
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

